#htb #AI #LLM

Использование ИИ/LLM локально дает множество преимуществ. Одна из основных причин, по которой люди используют локальный ИИ, — это конфиденциальность, поскольку для обработки всех данных моделью ИИ/LLM не требуется подключение к интернету. Локальная настройка также обеспечивает доступность в автономном режиме, что является важной особенностью для жителей регионов с нестабильным подключением к интернету.

Еще одним важным аспектом является то, что модель можно настраивать и дорабатывать, что повышает эффективность и соответствует нашим потребностям. Но у этого есть и недостаток, поскольку требуются локальные ресурсы ЦП/ГП, а старые видеокарты едва ли смогут справиться с огромным объемом вычислений, чтобы служить стабильным ресурсом для локального ИИ/LLM. Однако, с другой стороны, у вас не будет никаких дополнительных затрат, кроме тех, что связаны с производством электроэнергии.

---

## LM Studio

[Одним из самых эффективных и простых приложений, которое можно использовать для размещения локального ИИ/LLM, является LM Studio](https://lmstudio.ai/). Оно позволяет нам находить различные модели на разных ресурсах, таких как [Hugging Face](https://huggingface.co/), загружать их и позволять этим моделям решать наши задачи. Ещё одним преимуществом является то, что оно устраняет необходимость и зависимость от внешнего сервера, который в то же время позволяет нам использовать локальный сервер с ИИ/LLM для приложений на основе терминала. Оно также предлагает три различных режима:

- Пользователь
- Опытный Пользователь
- Разработчик

В режиме разработчика доступно наибольшее количество настроек и конфигураций. Давайте зайдем на их сайт и загрузим LM Studio.

![Домашняя страница LM Studio с возможностью загрузки таких инструментов ИИ, как Llama и DeepSeek.](https://academy.hackthebox.com/storage/modules/87/self1.png)

После загрузки давайте перейдём в каталог «Загрузки» и запустим его с помощью следующих команд:

  Автономный искусственный интеллект

```shell-session
cry0l1t3@ubuntu:~$ cd ~/Downloads
cry0l1t3@ubuntu:~$ ./LM-Studio
```

Появится новое окно, в котором вас попросят загрузить ваш первый LLM и установить его.

![Загрузка модели Llama, прогресс 17%, оптимизирована для многоязычного диалога.](https://academy.hackthebox.com/storage/modules/87/self2.png)

После загрузки LLM нам нужно будет указать LM Studio, какую модель использовать. В верхней части посередине вы увидите поле для выбора, нажав на которое вы увидите загруженные вами модели. В нижней части вы увидите режимы. Рекомендуется переключиться в режим `developer`, так как он включает большинство функций. Затем в правом верхнем углу вы увидите кнопку, которая открывает боковую панель с дополнительными параметрами конфигурации.

![Интерфейс LM Studio с чатом о Hack The Box Academy, использующий модель Llama.](https://academy.hackthebox.com/storage/modules/87/self3.png)

На левой боковой панели вы увидите кнопку поиска, которая откроет новое окно со списком доступных LLM. Не стесняйтесь просматривать список и проверять, какая модель подходит для вашего оборудования. LM Studio сообщит, соответствует ли ваше оборудование требованиям для конкретной LLM.

![Интерфейс LM Studio, отображающий поиск модели Gemma 3 4B QAT с возможностью загрузки.](https://academy.hackthebox.com/storage/modules/87/self4.png)

Когда мы нажимаем на кнопку «Оболочка» на левой боковой панели, мы видим опцию с настройками для запуска локального сервера API. Это позволяет нам отправлять запросы API через терминал или код, чтобы попросить локально размещённую LLM обработать определённую информацию.

![Интерфейс настроек LM Studio с отображением состояния сервера и информации о модели.](https://academy.hackthebox.com/storage/modules/87/self5.png)

Давайте запустим сервер и используем `curlie` для отправки простого запроса.

![Интерфейс терминала и LM Studio показывает команду curl для завершения чата с ответом в формате JSON, указывающим, что сегодня четверг.](https://academy.hackthebox.com/storage/modules/87/self6.png)

 